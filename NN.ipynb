{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'env (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch.quantization\n",
    "from tqdm import tqdm \n",
    "from torch.nn.utils import prune\n",
    "# import hls4ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager\n",
    "from cycler import cycler\n",
    "from matplotlib.font_manager import FontProperties\n",
    "font_path = '/vols/cms/hw423/Acc/Fonts/ImperialSansText-Regular.ttf'\n",
    "font_manager.fontManager.addfont(font_path)  # 添加字体路径\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()  # 获取字体名称\n",
    "plt.rcParams['font.family'] = font_name \n",
    "plt.rcParams['axes.prop_cycle'] = cycler(color=['#0000CD', '#7B68EE', '#000080', '#C71585', '#9467bd'])\n",
    "plt.rcParams['figure.facecolor'] ='#F5F5F5'\n",
    "plt.rcParams['axes.facecolor'] = '#F5F5F5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC(model,test_loader):\n",
    "    model.eval()\n",
    "    y_pre = []\n",
    "    y_labe = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            y_pre=np.append(y_pre,outputs.numpy())\n",
    "            y_labe=np.append(y_labe,labels.numpy())\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(y_labe, y_pre)\n",
    "    print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un-modified Model\n",
    "\n",
    "- Memory Calculation:\n",
    "1. Layer 0:\n",
    "    - Weights: 140 * 32 parameters\n",
    "    - Biases: 32 parameters\n",
    "    - Memory: M0 = (140 * 32 + 32) * 4 = 18048 bytes = 18048/1024 KB = 17.625 KB\n",
    "2. Layer 1: \n",
    "    - Weights: 32 * 16 parameters\n",
    "    - Biases: 16 parameters\n",
    "    - Memory: M1 = (32 * 16 + 16) * 4 = 2112 bytes = 2.0625 KB\n",
    "3. Layer 2:\n",
    "    - Weights: 16 * 1 parameters\n",
    "    - Biases: 1 parameters\n",
    "    - Memory: M2 = (16 * 1 + 1) * 4 = 68 bytes \n",
    "Memory Calculation for Parameters:\n",
    "1. Weights: In each layer is the production of the input and output sizes. For biases, it is the output size of the layer.\n",
    "2. Total Memory: Sum up all weights and biases across all layers and multiply by 4 bytes (for 32-bit precision).\n",
    "\n",
    "M = 18048 + 2112 + 68 bytes = 19.754 KB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('df.parquet')\n",
    "X = pd.read_parquet('x.parquet').values  \n",
    "df['proc'] = df['proc'].apply(lambda x: 1 if x != 0 else 0)\n",
    "y = df['proc'].values  \n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "layers = [1024, 512, 128]\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationModel, self).__init__()\n",
    "        self.layer0 = nn.Linear(X.shape[1], layers[0])\n",
    "        self.layer1 = nn.Linear(layers[0], layers[1])\n",
    "        self.layer2 = nn.Linear(layers[1], layers[2])\n",
    "        self.layer3 = nn.Linear(layers[2], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        timings = {}\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = torch.relu(self.layer0(x))\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        timings['fc_time'] = time.time() - start_time\n",
    "\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        \n",
    "        return x, timings['fc_time']\n",
    "model = BinaryClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, lr=0.001, num_epochs=50):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()  # Combines Sigmoid + BCELoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    timing = {'fc_time': 0}\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs, fc_time = model(inputs)\n",
    "            \n",
    "            # Accumulate the timing information\n",
    "            timing['fc_time'] += fc_time\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "    \n",
    "    return losses, timing, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 20/20 [25:50<00:00, 77.51s/it] \n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "losses, timing, model = train_model(model, train_loader, lr=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fc_time': 502.9939477443695}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `BinaryClassificationModel` is defined and accessible\n",
    "model = BinaryClassificationModel()\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91575\n",
      "Predict time: 6.018877029418945\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pre = []\n",
    "T = []\n",
    "for _ in range(200):\n",
    "    times = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            stt= time.time()\n",
    "            outputs, _ = model(inputs)\n",
    "            ent = time.time()\n",
    "            times+=ent-stt\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_pre.append(predicted)\n",
    "        T.append(times)\n",
    "T= np.array(T)\n",
    "accuracy0 = correct / total\n",
    "\n",
    "print(f'Accuracy: {accuracy0}')\n",
    "print(f'Predict time: {times}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([],bins=100,density =False)\n",
    "plt.hist(T,bins=20,density = False, label = 'Unquantised')\n",
    "# plt.xlim(0,0.2)\n",
    "# plt.xscale('log')\n",
    "plt.title('Inference time')\n",
    "plt.legend()\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('time')\n",
    "plt.savefig('vt_u.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9651988481557991\n"
     ]
    }
   ],
   "source": [
    "y_pre = []\n",
    "y_labe = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs, _ = model(inputs)\n",
    "        y_pre=np.append(y_pre,outputs.numpy())\n",
    "        y_labe=np.append(y_labe,labels.numpy())\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_labe, y_pre)\n",
    "print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lss = []\n",
    "for i in range(len(losses)):\n",
    "    lss.append(losses[i])\n",
    "plt.plot(lss)\n",
    "plt.title('BCE Loss')\n",
    "plt.savefig('Loss.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ptq = BinaryClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 20/20 [17:28<00:00, 52.43s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "losses, timing, model_p = train_model(model_ptq, train_loader, lr=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_p.state_dict(), 'model_p.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p = BinaryClassificationModel()\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_p.load_state_dict(torch.load('model_p.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModel(\n",
       "  (layer0): Linear(in_features=140, out_features=1024, bias=True)\n",
       "  (layer1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (layer2): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (layer3): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p.eval()\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model_p,  \n",
    "    {nn.Linear},  \n",
    "    dtype=torch.qint8\n",
    ")\n",
    "quantized_model.eval()\n",
    "QT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9083333333333333\n",
      "Predict time: 0.030799388885498047\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(200):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    Qtimes = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            stt = time.time()\n",
    "            outputs, _  = quantized_model(inputs)\n",
    "            ent = time.time()\n",
    "            Qtimes+=ent-stt\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    QT.append(Qtimes)\n",
    "QT=np.array(QT)\n",
    "accuracy2 = correct / total\n",
    "print(f'Accuracy = {accuracy2}')\n",
    "print(f'Predict time: {Qtimes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a,b,_= plt.hist(QT[QT<0.05],bins=50,density =False,label = 'Quantised')\n",
    "# plt.hist(T,bins=b,density = False, label = 'Unquantised')\n",
    "# plt.xlim(0.006,0.05)\n",
    "# plt.xscale('log')\n",
    "plt.title('Inference time')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.savefig('vt.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AUC(model,test_loader):\n",
    "    model.eval()\n",
    "    y_pre = []\n",
    "    y_labe = []\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs, _ = model(inputs)\n",
    "            y_pre=np.append(y_pre,outputs.numpy())\n",
    "            y_labe=np.append(y_labe,labels.numpy())\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(y_labe, y_pre)\n",
    "    print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9660662181765065\n"
     ]
    }
   ],
   "source": [
    "AUC(model_p,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModel(\n",
       "  (layer0): DynamicQuantizedLinear(in_features=140, out_features=1024, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (layer1): DynamicQuantizedLinear(in_features=1024, out_features=512, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (layer2): DynamicQuantizedLinear(in_features=512, out_features=128, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (layer3): DynamicQuantizedLinear(in_features=128, out_features=1, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.50390625"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of features for each Linear layer\n",
    "in_features = [140, 64, 32, 16]\n",
    "out_features = [64, 32, 16, 1]\n",
    "\n",
    "# Calculate the total memory for weights (in bytes)\n",
    "# Since these are not quantized, we assume float32, which is 4 bytes per weight\n",
    "total_weights_memory = sum(in_f * out_f * 4 for in_f, out_f in zip(in_features, out_features))\n",
    "\n",
    "# Calculate the total memory for biases (in bytes)\n",
    "# Each bias also uses 4 bytes (for float32)\n",
    "total_biases_memory = sum(out_f * 4 for out_f in out_features)\n",
    "\n",
    "# Total memory used by the model (in bytes)\n",
    "total_model_memory_bytes = total_weights_memory + total_biases_memory\n",
    "\n",
    "total_model_memory_bytes / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "layers = [64, 32, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer0): Linear(\n",
       "    in_features=140, out_features=64, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Linear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Linear(\n",
       "    in_features=32, out_features=16, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Linear(\n",
       "    in_features=16, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryClassificationModelQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationModelQ, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.layer0 = nn.Linear(X.shape[1], layers[0])\n",
    "        self.layer1 = nn.Linear(layers[0], layers[1])\n",
    "        self.layer2 = nn.Linear(layers[1], layers[2])\n",
    "        self.layer3 = nn.Linear(layers[2], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        timings = {}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        x = self.quant(x)\n",
    "        timings['quant_time'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = torch.relu(self.layer0(x))\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        timings['fc_time'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        x = self.dequant(x)\n",
    "        timings['dequant_time'] = time.time() - start_time\n",
    "\n",
    "        return x, timings['quant_time'], timings['fc_time'], timings['dequant_time']\n",
    "\n",
    "modelQ = BinaryClassificationModelQ()\n",
    "modelQ.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(modelQ, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, lr=0.001, num_epochs=50):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()  # Combines Sigmoid + BCELoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    timing = {'quant_time': 0, 'fc_time': 0, 'dequant_time': 0}\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs, quant_time, fc_time,dequant_time = model(inputs)\n",
    "            \n",
    "            # Accumulate the timing information\n",
    "            timing['quant_time'] += quant_time\n",
    "            timing['fc_time'] += fc_time\n",
    "            timing['dequant_time'] += dequant_time\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "    \n",
    "    return losses, timing, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 1/20 [00:30<09:41, 30.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6770993889646327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 2/20 [00:58<08:45, 29.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.6300197364167964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 3/20 [01:27<08:13, 29.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.5529150414340039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 4/20 [01:57<07:47, 29.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.5231283563882747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 5/20 [02:17<06:28, 25.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.5116599833077573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 6/20 [02:34<05:22, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.511191650590998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 7/20 [02:54<04:45, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.5050710065567747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 8/20 [03:10<04:02, 20.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.5006748726393314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 9/20 [03:29<03:36, 19.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.4960322785884776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 10/20 [03:47<03:12, 19.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.4920148599020978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 11/20 [04:05<02:48, 18.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.48331573161673036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 12/20 [04:24<02:32, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.460729274343937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 13/20 [04:42<02:10, 18.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.42578001035020707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 14/20 [04:59<01:48, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.364459198840121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  75%|███████▌  | 15/20 [05:17<01:30, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.3236592754404596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 16/20 [05:34<01:11, 17.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.3008267732059702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  85%|████████▌ | 17/20 [05:52<00:53, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.27787219796408996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 18/20 [06:11<00:36, 18.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 1.5259761778598135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  95%|█████████▌| 19/20 [06:28<00:17, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 2.071602241790041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 20/20 [06:45<00:00, 20.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 2.071371963683595\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "losses, timing, model_qat = train_model(modelQ, train_loader, lr=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quant_time': 55.34764337539673, 'fc_time': 153.7611107826233, 'dequant_time': 1.524585485458374}\n"
     ]
    }
   ],
   "source": [
    "print(timing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, val_loader, lr=0.001, num_epochs=50, patience=5):\n",
    "    model.train()\n",
    "    criterion = nn.BCELoss()  # Combines Sigmoid + BCELoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    val_losses = []  # To track validation loss for early stopping\n",
    "    timing = {'quant_time': 0, 'fc_time': 0, 'dequant_time': 0}\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0  # Track epochs with no improvement\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        model.train()  # Ensure model is in training mode\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs, quant_time, fc_time,dequant_time = model(inputs)\n",
    "            \n",
    "            # Accumulate the timing information\n",
    "            timing['quant_time'] += quant_time\n",
    "            timing['fc_time'] += fc_time\n",
    "            timing['dequant_time'] += dequant_time\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Switch to evaluation mode for validation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs, _, _, _ = model(inputs)\n",
    "                loss = criterion(outputs.squeeze(), labels.float())\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model\n",
    "            best_model = model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break  # Stop training\n",
    "\n",
    "    return losses, val_losses, timing, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6632636056301442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 1/20 [00:20<06:24, 20.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.6749265938997269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.7317377417645556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 2/20 [00:40<06:03, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.8558924421668053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.1041346059200612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 3/20 [01:03<06:05, 21.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.4638294130563736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 1.8746333566117794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 4/20 [01:24<05:41, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.087880219022433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2.07213978691304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 5/20 [01:45<05:17, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.0878799011309943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 2.071257009151134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 5/20 [02:05<06:16, 25.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.087879573305448\n",
      "Early stopping triggered after 6 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "L, V, T, B = train_model(modelQ, train_loader, test_loader, lr=0.001, num_epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model_qat.state_dict(), 'model_q.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BinaryClassificationModelQ:\n\tUnexpected key(s) in state_dict: \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\", \"layer0.weight_fake_quant.fake_quant_enabled\", \"layer0.weight_fake_quant.observer_enabled\", \"layer0.weight_fake_quant.scale\", \"layer0.weight_fake_quant.zero_point\", \"layer0.weight_fake_quant.activation_post_process.eps\", \"layer0.weight_fake_quant.activation_post_process.min_val\", \"layer0.weight_fake_quant.activation_post_process.max_val\", \"layer0.activation_post_process.fake_quant_enabled\", \"layer0.activation_post_process.observer_enabled\", \"layer0.activation_post_process.scale\", \"layer0.activation_post_process.zero_point\", \"layer0.activation_post_process.activation_post_process.eps\", \"layer0.activation_post_process.activation_post_process.min_val\", \"layer0.activation_post_process.activation_post_process.max_val\", \"layer1.weight_fake_quant.fake_quant_enabled\", \"layer1.weight_fake_quant.observer_enabled\", \"layer1.weight_fake_quant.scale\", \"layer1.weight_fake_quant.zero_point\", \"layer1.weight_fake_quant.activation_post_process.eps\", \"layer1.weight_fake_quant.activation_post_process.min_val\", \"layer1.weight_fake_quant.activation_post_process.max_val\", \"layer1.activation_post_process.fake_quant_enabled\", \"layer1.activation_post_process.observer_enabled\", \"layer1.activation_post_process.scale\", \"layer1.activation_post_process.zero_point\", \"layer1.activation_post_process.activation_post_process.eps\", \"layer1.activation_post_process.activation_post_process.min_val\", \"layer1.activation_post_process.activation_post_process.max_val\", \"layer2.weight_fake_quant.fake_quant_enabled\", \"layer2.weight_fake_quant.observer_enabled\", \"layer2.weight_fake_quant.scale\", \"layer2.weight_fake_quant.zero_point\", \"layer2.weight_fake_quant.activation_post_process.eps\", \"layer2.weight_fake_quant.activation_post_process.min_val\", \"layer2.weight_fake_quant.activation_post_process.max_val\", \"layer2.activation_post_process.fake_quant_enabled\", \"layer2.activation_post_process.observer_enabled\", \"layer2.activation_post_process.scale\", \"layer2.activation_post_process.zero_point\", \"layer2.activation_post_process.activation_post_process.eps\", \"layer2.activation_post_process.activation_post_process.min_val\", \"layer2.activation_post_process.activation_post_process.max_val\", \"layer3.weight_fake_quant.fake_quant_enabled\", \"layer3.weight_fake_quant.observer_enabled\", \"layer3.weight_fake_quant.scale\", \"layer3.weight_fake_quant.zero_point\", \"layer3.weight_fake_quant.activation_post_process.eps\", \"layer3.weight_fake_quant.activation_post_process.min_val\", \"layer3.weight_fake_quant.activation_post_process.max_val\", \"layer3.activation_post_process.fake_quant_enabled\", \"layer3.activation_post_process.observer_enabled\", \"layer3.activation_post_process.scale\", \"layer3.activation_post_process.zero_point\", \"layer3.activation_post_process.activation_post_process.eps\", \"layer3.activation_post_process.activation_post_process.min_val\", \"layer3.activation_post_process.activation_post_process.max_val\", \"sigmoid.activation_post_process.fake_quant_enabled\", \"sigmoid.activation_post_process.observer_enabled\", \"sigmoid.activation_post_process.scale\", \"sigmoid.activation_post_process.zero_point\", \"sigmoid.activation_post_process.activation_post_process.scale\", \"sigmoid.activation_post_process.activation_post_process.zero_point\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the model \u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_q \u001b[38;5;241m=\u001b[39m BinaryClassificationModelQ()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_q.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BinaryClassificationModelQ:\n\tUnexpected key(s) in state_dict: \"quant.activation_post_process.fake_quant_enabled\", \"quant.activation_post_process.observer_enabled\", \"quant.activation_post_process.scale\", \"quant.activation_post_process.zero_point\", \"quant.activation_post_process.activation_post_process.eps\", \"quant.activation_post_process.activation_post_process.min_val\", \"quant.activation_post_process.activation_post_process.max_val\", \"layer0.weight_fake_quant.fake_quant_enabled\", \"layer0.weight_fake_quant.observer_enabled\", \"layer0.weight_fake_quant.scale\", \"layer0.weight_fake_quant.zero_point\", \"layer0.weight_fake_quant.activation_post_process.eps\", \"layer0.weight_fake_quant.activation_post_process.min_val\", \"layer0.weight_fake_quant.activation_post_process.max_val\", \"layer0.activation_post_process.fake_quant_enabled\", \"layer0.activation_post_process.observer_enabled\", \"layer0.activation_post_process.scale\", \"layer0.activation_post_process.zero_point\", \"layer0.activation_post_process.activation_post_process.eps\", \"layer0.activation_post_process.activation_post_process.min_val\", \"layer0.activation_post_process.activation_post_process.max_val\", \"layer1.weight_fake_quant.fake_quant_enabled\", \"layer1.weight_fake_quant.observer_enabled\", \"layer1.weight_fake_quant.scale\", \"layer1.weight_fake_quant.zero_point\", \"layer1.weight_fake_quant.activation_post_process.eps\", \"layer1.weight_fake_quant.activation_post_process.min_val\", \"layer1.weight_fake_quant.activation_post_process.max_val\", \"layer1.activation_post_process.fake_quant_enabled\", \"layer1.activation_post_process.observer_enabled\", \"layer1.activation_post_process.scale\", \"layer1.activation_post_process.zero_point\", \"layer1.activation_post_process.activation_post_process.eps\", \"layer1.activation_post_process.activation_post_process.min_val\", \"layer1.activation_post_process.activation_post_process.max_val\", \"layer2.weight_fake_quant.fake_quant_enabled\", \"layer2.weight_fake_quant.observer_enabled\", \"layer2.weight_fake_quant.scale\", \"layer2.weight_fake_quant.zero_point\", \"layer2.weight_fake_quant.activation_post_process.eps\", \"layer2.weight_fake_quant.activation_post_process.min_val\", \"layer2.weight_fake_quant.activation_post_process.max_val\", \"layer2.activation_post_process.fake_quant_enabled\", \"layer2.activation_post_process.observer_enabled\", \"layer2.activation_post_process.scale\", \"layer2.activation_post_process.zero_point\", \"layer2.activation_post_process.activation_post_process.eps\", \"layer2.activation_post_process.activation_post_process.min_val\", \"layer2.activation_post_process.activation_post_process.max_val\", \"layer3.weight_fake_quant.fake_quant_enabled\", \"layer3.weight_fake_quant.observer_enabled\", \"layer3.weight_fake_quant.scale\", \"layer3.weight_fake_quant.zero_point\", \"layer3.weight_fake_quant.activation_post_process.eps\", \"layer3.weight_fake_quant.activation_post_process.min_val\", \"layer3.weight_fake_quant.activation_post_process.max_val\", \"layer3.activation_post_process.fake_quant_enabled\", \"layer3.activation_post_process.observer_enabled\", \"layer3.activation_post_process.scale\", \"layer3.activation_post_process.zero_point\", \"layer3.activation_post_process.activation_post_process.eps\", \"layer3.activation_post_process.activation_post_process.min_val\", \"layer3.activation_post_process.activation_post_process.max_val\", \"sigmoid.activation_post_process.fake_quant_enabled\", \"sigmoid.activation_post_process.observer_enabled\", \"sigmoid.activation_post_process.scale\", \"sigmoid.activation_post_process.zero_point\", \"sigmoid.activation_post_process.activation_post_process.scale\", \"sigmoid.activation_post_process.activation_post_process.zero_point\". "
     ]
    }
   ],
   "source": [
    "# load the model \n",
    "model_q = BinaryClassificationModelQ()\n",
    "model_q.load_state_dict(torch.load('model_q.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2662]), zero_point=tensor([30], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-8.115145683288574, max_val=25.695375442504883)\n",
       "    )\n",
       "  )\n",
       "  (layer0): Linear(\n",
       "    in_features=140, out_features=1024, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.1000e-05, 6.1000e-05, 6.1000e-05,  ..., 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05]), zero_point=tensor([0, 0, 0,  ..., 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-3.7869e-31, -7.1353e-31, -2.4794e-03,  ..., -4.9667e-31,\n",
       "                -6.9559e-31, -7.9265e-31]), max_val=tensor([4.6209e-31, 9.3804e-31, 1.0156e-03,  ..., 5.2258e-31, 6.8739e-31,\n",
       "                9.2618e-31])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0485]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0672346253265408e-12, max_val=6.155506610870361)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Linear(\n",
       "    in_features=1024, out_features=512, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 1.3945e-04, 6.1000e-05, 6.1000e-05, 1.1184e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.1346e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              2.1666e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 1.9583e-04, 6.1000e-05, 2.2417e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 2.2435e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 1.9205e-04, 2.1930e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.2110e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 1.2277e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              2.2394e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 2.4286e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 1.8343e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.7846e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.8290e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              1.9532e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.2277e-04,\n",
       "              6.1000e-05, 1.1347e-04, 6.1000e-05, 6.1000e-05, 2.0138e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 2.4566e-04, 6.1000e-05, 2.4455e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 1.2277e-04, 2.4408e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 2.3609e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 1.9206e-04, 6.1000e-05, 6.1000e-05, 2.1221e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 2.4421e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 1.3938e-04, 2.2686e-04, 2.3724e-04, 7.1392e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              8.4080e-05, 6.1000e-05, 6.1000e-05, 1.3947e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.2109e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              1.1346e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.2109e-04, 2.4038e-04,\n",
       "              9.3548e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 2.1962e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              2.0526e-04, 6.1000e-05, 2.4429e-04, 6.1000e-05, 6.1000e-05, 1.1776e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 2.3095e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              1.6768e-04, 6.1000e-05, 2.4707e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 2.4708e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 1.3954e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.5968e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.0801e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-4.6764e-13, -1.7536e-31, -2.6170e-31, -4.5930e-12, -2.6023e-31,\n",
       "                -2.6181e-31, -5.0588e-13, -2.6352e-31, -2.6274e-31, -2.6312e-28,\n",
       "                -1.7672e-28, -2.5096e-31, -4.6741e-32, -3.3744e-32, -3.3721e-32,\n",
       "                -2.6009e-31, -2.4136e-31, -4.1289e-31, -2.6803e-31, -2.6514e-31,\n",
       "                -1.4365e-31, -1.7781e-31, -3.5018e-13, -2.5581e-31, -1.2640e-23,\n",
       "                -2.6151e-31, -3.3655e-32, -2.4949e-31, -2.5723e-31, -2.3491e-31,\n",
       "                -2.5279e-31, -2.6914e-12, -2.5813e-31, -3.8060e-13, -2.5705e-31,\n",
       "                -2.5820e-31, -2.5870e-31, -5.0931e-13, -1.8585e-12, -1.2579e-31,\n",
       "                -2.5998e-31, -1.9118e-12, -9.0179e-32, -3.3658e-32, -2.6222e-31,\n",
       "                -7.9879e-32, -2.5056e-31, -2.5407e-31, -1.1804e-31, -2.6386e-31,\n",
       "                -2.5540e-31, -2.5223e-31, -2.0112e-12, -3.3728e-32, -2.6103e-31,\n",
       "                -2.5756e-23, -2.5100e-31, -1.5616e-31, -1.2840e-31, -3.3486e-23,\n",
       "                -2.6241e-31, -4.7067e-13, -2.6223e-31, -4.0150e-31, -3.3411e-32,\n",
       "                -1.7517e-31, -2.6423e-31, -5.0784e-13, -2.5774e-31, -2.6353e-31,\n",
       "                -1.4519e-31, -3.3674e-32, -2.2330e-31, -1.0816e-31, -2.5772e-31,\n",
       "                -2.5939e-31, -2.5332e-31, -2.2672e-31, -2.5964e-31, -2.6043e-31,\n",
       "                -4.0935e-31, -3.3636e-32, -3.3693e-32, -3.6156e-13, -3.3713e-32,\n",
       "                -4.1049e-13, -2.5629e-31, -4.7418e-31, -2.5483e-31, -2.9739e-18,\n",
       "                -4.5623e-31, -2.5575e-31, -1.6092e-31, -4.8988e-31, -2.5015e-31,\n",
       "                -2.6441e-31, -1.3414e-14, -2.5662e-31, -2.6439e-31, -1.9990e-31,\n",
       "                -1.4581e-23, -2.6594e-31, -1.1913e-11, -2.6408e-31, -2.4465e-31,\n",
       "                -2.5945e-31, -2.5524e-31, -2.5978e-31, -1.5174e-31, -2.6121e-31,\n",
       "                -4.1945e-31, -3.3702e-32, -2.5807e-31, -8.6656e-32, -3.3732e-32,\n",
       "                -2.6513e-31, -2.5380e-31, -2.6012e-31, -3.3733e-32, -2.2750e-31,\n",
       "                -2.6291e-31, -2.1400e-31, -3.3539e-12, -3.3745e-32, -1.5264e-11,\n",
       "                -2.4118e-31, -1.1552e-31, -2.6201e-31, -2.5026e-31, -1.2938e-11,\n",
       "                -1.7366e-31, -1.4964e-23, -1.6920e-28, -5.0872e-13, -2.5087e-31,\n",
       "                -3.3748e-32, -2.2585e-31, -2.6403e-31, -3.2093e-18, -2.0300e-31,\n",
       "                -1.9884e-31, -2.7088e-18, -2.6103e-31, -1.3539e-31, -2.5222e-31,\n",
       "                -4.5283e-19, -3.1310e-12, -1.3670e-11, -3.3747e-32, -2.6017e-31,\n",
       "                -3.1134e-23, -2.0767e-31, -1.5831e-31, -2.4477e-31, -3.3693e-32,\n",
       "                -2.6021e-31, -2.5028e-31, -4.6401e-13, -2.4355e-31, -2.5865e-31,\n",
       "                -2.1120e-12, -2.6321e-31, -2.2665e-31, -1.2710e-28, -4.5707e-31,\n",
       "                -2.0539e-12, -3.3640e-32, -1.4342e-31, -2.5656e-31, -3.3618e-32,\n",
       "                -3.3687e-32, -5.5005e-20, -3.3746e-32, -1.3239e-31, -1.4856e-11,\n",
       "                -2.6411e-31, -2.3197e-31, -4.1956e-31, -7.7251e-32, -3.3602e-23,\n",
       "                -2.3976e-31, -2.4857e-31, -1.2944e-11, -1.5968e-31, -2.1517e-31,\n",
       "                -4.4534e-31, -3.3637e-32, -2.6313e-31, -3.3760e-32, -2.6315e-31,\n",
       "                -2.0152e-31, -1.2928e-23, -1.5688e-31, -2.5704e-31, -2.3398e-31,\n",
       "                -4.0551e-31, -2.5512e-31, -2.0693e-31, -2.5899e-31, -2.6521e-31,\n",
       "                -1.3969e-31, -3.3753e-32, -2.5583e-31, -1.6588e-28, -1.8176e-31,\n",
       "                -4.7852e-31, -1.9534e-31, -2.5713e-12, -1.9191e-31, -2.4005e-31,\n",
       "                -5.0537e-13, -1.3327e-31, -3.3712e-32, -2.4804e-31, -5.3998e-15,\n",
       "                -4.2833e-13, -3.3681e-32, -2.4736e-31, -2.6062e-31, -5.1149e-13,\n",
       "                -2.2323e-31, -2.0160e-31, -4.2117e-13, -1.5595e-31, -2.6499e-31,\n",
       "                -2.2239e-31, -1.3235e-31, -2.6379e-31, -2.1832e-28, -1.7813e-31,\n",
       "                -1.0740e-18, -1.7863e-31, -2.6193e-31, -1.7642e-31, -3.3727e-32,\n",
       "                -4.3618e-13, -2.0990e-31, -3.3701e-32, -2.3717e-31, -1.8183e-12,\n",
       "                -2.5485e-31, -2.4812e-31, -2.6166e-31, -2.6463e-31, -1.9299e-12,\n",
       "                -2.6158e-31, -1.3911e-31, -2.6141e-31, -2.3080e-31, -2.7440e-12,\n",
       "                -2.5756e-31, -2.5733e-31, -3.2879e-12, -3.3665e-32, -1.3901e-31,\n",
       "                -2.6211e-31, -2.6241e-31, -2.0925e-12, -2.2684e-31, -2.0000e-12,\n",
       "                -2.4037e-31, -3.3744e-32, -3.4537e-12, -1.3967e-31, -1.0494e-12,\n",
       "                -2.5085e-31, -6.5056e-12, -1.9023e-19, -3.0251e-12, -2.5881e-31,\n",
       "                -2.3353e-31, -1.8970e-31, -2.6437e-31, -2.5118e-31, -2.5988e-31,\n",
       "                -2.1399e-31, -2.6287e-31, -2.0923e-12, -7.9711e-12, -3.3740e-32,\n",
       "                -3.4103e-12, -2.6384e-31, -3.3663e-32, -3.3746e-32, -2.5397e-31,\n",
       "                -1.5685e-11, -2.6313e-31, -2.6252e-31, -2.6237e-31, -2.0348e-31,\n",
       "                -2.9445e-12, -1.6255e-13, -2.3356e-31, -7.0804e-12, -2.6031e-31,\n",
       "                -2.6296e-31, -2.6098e-31, -2.5697e-31, -2.5442e-31, -3.9969e-12,\n",
       "                -1.9441e-31, -2.2769e-31, -2.4286e-31, -2.6349e-31, -2.2091e-31,\n",
       "                -1.8263e-31, -3.3414e-32, -1.7241e-31, -2.6297e-31, -4.3820e-12,\n",
       "                -2.3142e-31, -2.4881e-31, -2.6185e-31, -2.6206e-31, -3.3574e-32,\n",
       "                -2.4929e-31, -1.4347e-31, -2.5993e-31, -2.5864e-31, -2.2693e-31,\n",
       "                -2.5642e-31, -2.5241e-31, -2.6048e-31, -2.5015e-31, -4.2879e-31,\n",
       "                -1.8663e-12, -1.4958e-11, -5.8251e-12, -5.8789e-12, -7.9322e-32,\n",
       "                -2.6408e-31, -4.5581e-13, -7.1468e-13, -3.3647e-32, -3.3656e-32,\n",
       "                -5.1132e-13, -5.9332e-12, -3.9353e-13, -2.5148e-31, -1.8629e-12,\n",
       "                -6.3452e-13, -2.6423e-31, -1.9793e-31, -1.8879e-31, -2.5901e-31,\n",
       "                -2.5861e-31, -2.1307e-12, -3.3504e-32, -3.3749e-32, -2.6319e-31,\n",
       "                -2.5574e-31, -2.5918e-31, -2.6133e-31, -2.5711e-31, -2.5880e-31,\n",
       "                -2.4970e-31, -2.5057e-31, -5.0495e-13, -1.9873e-28, -1.3468e-31,\n",
       "                -1.5588e-31, -3.3718e-32, -2.6453e-31, -2.4740e-31, -9.1879e-32,\n",
       "                -2.3730e-31, -2.0114e-12, -1.5642e-28, -1.7852e-31, -3.8149e-13,\n",
       "                -2.1289e-12, -8.2982e-12, -5.0498e-12, -3.3744e-32, -2.5803e-31,\n",
       "                -2.5503e-31, -4.0915e-13, -3.3705e-32, -2.6085e-31, -3.3760e-32,\n",
       "                -2.5103e-31, -2.5745e-31, -3.3752e-32, -2.4086e-31, -2.0432e-31,\n",
       "                -2.5580e-31, -1.0184e-31, -2.5183e-31, -2.4917e-31, -1.4805e-23,\n",
       "                -1.3597e-31, -2.6520e-31, -2.6452e-31, -2.5704e-31, -2.6373e-31,\n",
       "                -3.3761e-32, -2.4720e-14, -1.4608e-28, -1.2853e-11, -2.0011e-28,\n",
       "                -2.5825e-31, -2.6163e-31, -6.1824e-12, -1.2870e-31, -3.0426e-12,\n",
       "                -2.6422e-31, -2.3190e-31, -2.0059e-12, -2.6139e-31, -2.4750e-31,\n",
       "                -2.5153e-31, -4.4839e-12, -2.5202e-31, -2.6569e-31, -1.4467e-23,\n",
       "                -2.6238e-31, -2.4726e-31, -4.8547e-13, -1.9450e-31, -2.6029e-12,\n",
       "                -3.0921e-23, -4.0080e-13, -2.5609e-31, -3.7984e-13, -2.1612e-31,\n",
       "                -5.1130e-13, -2.5075e-31, -1.5405e-11, -4.3659e-13, -3.7017e-13,\n",
       "                -2.5687e-31, -2.6239e-31, -1.0087e-12, -2.5885e-31, -8.4236e-12,\n",
       "                -2.5313e-31, -4.9559e-13, -2.6353e-31, -3.3733e-32, -3.2025e-12,\n",
       "                -3.2977e-18, -3.3547e-32, -1.8282e-31, -2.6239e-31, -3.3736e-32,\n",
       "                -1.9704e-31, -1.8990e-28, -2.5399e-31, -1.8735e-31, -7.6889e-12,\n",
       "                -2.0638e-31, -4.1798e-31, -3.3756e-32, -2.5059e-31, -2.0514e-31,\n",
       "                -3.3728e-32, -1.7736e-31, -2.2679e-31, -2.5393e-31, -2.4205e-31,\n",
       "                -2.3339e-31, -2.5183e-31, -1.5956e-13, -3.3691e-32, -2.6047e-31,\n",
       "                -2.3711e-12, -2.5526e-31, -2.4775e-31, -2.6192e-31, -2.6151e-31,\n",
       "                -2.5734e-31, -2.4123e-31, -2.5611e-31, -2.3551e-31, -2.9757e-23,\n",
       "                -2.5413e-31, -1.0247e-31, -2.5846e-31, -2.3939e-31, -2.5731e-31,\n",
       "                -2.5988e-31, -2.6313e-31, -3.3750e-32, -2.6238e-31, -2.6134e-31,\n",
       "                -2.5586e-31, -2.2588e-31, -1.4682e-12, -2.6037e-31, -2.6271e-31,\n",
       "                -2.3444e-31, -4.5508e-12, -3.3757e-32, -2.5513e-31, -9.0413e-32,\n",
       "                -2.6216e-31, -2.3390e-31, -2.5756e-31, -1.4236e-23, -3.3383e-18,\n",
       "                -2.6407e-31, -1.8797e-31, -4.2219e-13, -2.5395e-31, -5.7336e-12,\n",
       "                -1.9401e-31, -2.6065e-31, -2.4317e-31, -2.1285e-12, -5.0437e-19,\n",
       "                -5.1053e-13, -3.3733e-32]), max_val=tensor([4.6239e-13, 3.7281e-31, 1.8526e-31, 5.0070e-12, 1.8270e-31, 2.0529e-31,\n",
       "                3.6610e-13, 2.3036e-31, 2.3322e-31, 3.6975e-28, 1.7521e-28, 1.8197e-31,\n",
       "                4.1590e-32, 3.3752e-32, 3.3695e-32, 1.8218e-31, 1.6860e-31, 3.5079e-30,\n",
       "                6.2676e-31, 2.3408e-31, 1.1279e-31, 6.1800e-31, 3.1756e-13, 2.0230e-31,\n",
       "                1.2974e-23, 2.1461e-31, 3.3715e-32, 2.4466e-31, 1.7732e-31, 2.0512e-31,\n",
       "                1.7805e-31, 5.7097e-03, 2.4656e-31, 4.3815e-13, 1.9199e-31, 2.3837e-31,\n",
       "                2.0459e-31, 3.6010e-13, 1.7710e-02, 1.9663e-31, 2.1635e-31, 1.4203e-02,\n",
       "                1.2985e-31, 3.3753e-32, 1.8569e-31, 1.0568e-31, 2.4823e-31, 2.5011e-31,\n",
       "                1.0216e-31, 1.8057e-31, 1.7466e-31, 2.4773e-31, 1.4409e-02, 3.3537e-32,\n",
       "                1.8051e-31, 2.1359e-23, 1.7355e-31, 1.8056e-31, 1.9307e-31, 5.6678e-04,\n",
       "                1.8194e-31, 4.4955e-13, 2.1068e-31, 3.3473e-30, 3.3558e-32, 4.0067e-31,\n",
       "                2.4084e-31, 3.6974e-13, 2.4669e-31, 1.9065e-31, 1.0341e-31, 3.3695e-32,\n",
       "                1.5993e-31, 8.0911e-32, 2.4304e-31, 1.8054e-31, 1.7556e-31, 2.6889e-31,\n",
       "                2.5298e-31, 2.0703e-31, 4.9740e-30, 3.3754e-32, 3.3673e-32, 3.2347e-13,\n",
       "                3.3704e-32, 4.5735e-13, 2.4028e-31, 3.3579e-30, 1.8061e-31, 2.8084e-18,\n",
       "                2.8501e-30, 2.4298e-31, 2.3136e-31, 2.8425e-30, 1.7502e-31, 2.5661e-31,\n",
       "                5.6461e-04, 2.3344e-31, 2.1919e-31, 1.7634e-31, 1.1573e-23, 2.4980e-31,\n",
       "                2.7516e-02, 2.3736e-31, 1.6356e-31, 2.2154e-31, 2.5256e-31, 2.1083e-31,\n",
       "                2.3482e-31, 2.2676e-31, 3.3410e-30, 3.3632e-32, 1.9545e-31, 1.1136e-31,\n",
       "                3.3622e-32, 2.4110e-31, 1.9811e-31, 2.5859e-31, 3.3708e-32, 3.9536e-31,\n",
       "                2.0948e-31, 1.4787e-31, 2.4870e-02, 3.3735e-32, 2.8470e-02, 1.6810e-31,\n",
       "                1.6724e-31, 2.4301e-31, 1.8460e-31, 2.8493e-02, 4.8789e-31, 1.4349e-23,\n",
       "                1.9501e-28, 3.6403e-13, 2.0911e-31, 3.3696e-32, 3.0507e-31, 1.8235e-31,\n",
       "                2.7298e-18, 5.6569e-31, 6.4739e-31, 2.7109e-18, 2.5308e-31, 1.8903e-31,\n",
       "                2.4327e-31, 5.6428e-04, 2.4391e-02, 2.7851e-02, 3.3691e-32, 1.7888e-31,\n",
       "                2.1820e-23, 2.2732e-31, 2.7075e-31, 1.7841e-31, 3.3206e-32, 2.3313e-31,\n",
       "                1.7491e-31, 4.3967e-13, 7.4542e-31, 2.2161e-31, 1.5380e-02, 2.2932e-31,\n",
       "                1.6115e-31, 1.6699e-28, 3.5106e-30, 1.5592e-02, 3.3594e-32, 1.7261e-31,\n",
       "                1.9687e-31, 3.3595e-32, 3.3643e-32, 5.6700e-04, 3.3753e-32, 9.7972e-32,\n",
       "                2.8440e-02, 2.4580e-31, 2.6127e-31, 2.9949e-30, 7.3282e-32, 5.6686e-04,\n",
       "                1.7341e-31, 2.0968e-31, 3.0844e-02, 2.5362e-31, 2.3878e-31, 2.9362e-30,\n",
       "                3.3738e-32, 2.4646e-31, 3.3710e-32, 2.3702e-31, 2.3834e-31, 1.2819e-23,\n",
       "                2.2765e-31, 2.3426e-31, 4.1518e-31, 3.0440e-30, 2.5676e-31, 1.6090e-31,\n",
       "                1.9294e-31, 2.5382e-31, 1.9970e-31, 3.3689e-32, 1.7873e-31, 1.9479e-28,\n",
       "                3.1461e-31, 2.8070e-30, 5.3009e-31, 2.3296e-02, 7.6563e-31, 1.7452e-31,\n",
       "                3.9594e-13, 2.0612e-31, 3.3693e-32, 2.4560e-31, 1.8457e-03, 4.7340e-13,\n",
       "                3.3628e-32, 2.0213e-31, 2.3620e-31, 3.6355e-13, 1.5259e-31, 7.6487e-31,\n",
       "                2.7426e-03, 2.3847e-31, 2.6270e-31, 2.6154e-31, 1.0748e-31, 2.1423e-31,\n",
       "                1.9468e-28, 3.9784e-31, 1.8458e-03, 3.8624e-31, 1.9854e-31, 4.1892e-31,\n",
       "                3.3759e-32, 4.3088e-13, 1.4346e-31, 3.3747e-32, 3.5660e-31, 2.2664e-02,\n",
       "                2.2406e-31, 1.9278e-31, 2.2745e-31, 2.4427e-31, 2.3228e-02, 1.8627e-31,\n",
       "                2.0302e-31, 1.8366e-31, 1.8572e-31, 5.7091e-03, 1.7808e-31, 2.2020e-31,\n",
       "                2.4806e-02, 3.3593e-32, 1.4700e-31, 2.1376e-31, 2.3926e-31, 1.5592e-02,\n",
       "                2.3681e-31, 1.4411e-02, 1.7329e-31, 3.3744e-32, 2.5576e-02, 1.8289e-31,\n",
       "                1.2913e-12, 3.1966e-31, 3.1199e-02, 5.6674e-04, 3.1058e-02, 1.9482e-31,\n",
       "                1.6921e-31, 1.3032e-31, 2.4101e-31, 2.2992e-31, 2.3492e-31, 1.5580e-31,\n",
       "                1.9459e-31, 1.5592e-02, 3.0998e-02, 3.3749e-32, 4.3760e-12, 2.3815e-31,\n",
       "                3.3740e-32, 3.3742e-32, 2.3218e-31, 2.9983e-02, 2.2734e-31, 1.8086e-31,\n",
       "                2.1616e-31, 1.4045e-31, 2.4392e-02, 2.0102e-03, 3.5646e-31, 2.6950e-02,\n",
       "                2.5887e-31, 2.0739e-31, 1.8275e-31, 1.7747e-31, 2.5757e-31, 3.1015e-02,\n",
       "                1.4251e-31, 1.6483e-31, 2.1806e-31, 2.5474e-31, 1.6030e-31, 1.4337e-31,\n",
       "                3.3658e-32, 1.2831e-31, 1.9860e-31, 7.0147e-03, 3.3316e-31, 1.7442e-31,\n",
       "                2.1061e-31, 2.4132e-31, 3.3685e-32, 1.8850e-31, 2.0571e-31, 2.3624e-31,\n",
       "                1.8073e-31, 1.7175e-31, 2.3488e-31, 1.7462e-31, 1.7824e-31, 2.3215e-31,\n",
       "                5.1886e-30, 1.7701e-02, 2.8812e-02, 3.0129e-02, 9.0667e-03, 6.4570e-32,\n",
       "                2.2795e-31, 4.3066e-13, 3.1032e-03, 3.3645e-32, 3.3521e-32, 3.7600e-13,\n",
       "                1.0678e-02, 3.9836e-13, 1.7390e-31, 1.7713e-02, 3.1017e-03, 2.2507e-31,\n",
       "                1.5565e-31, 6.2527e-31, 2.5739e-31, 1.7525e-31, 1.5379e-02, 3.3697e-32,\n",
       "                3.3742e-32, 2.3647e-31, 2.2182e-31, 1.8557e-31, 2.3642e-31, 2.3110e-31,\n",
       "                1.8232e-31, 1.7066e-31, 1.8342e-31, 3.8914e-13, 1.9454e-28, 1.8649e-31,\n",
       "                2.2311e-31, 3.3598e-32, 2.3370e-31, 2.1069e-31, 1.1670e-31, 1.6656e-31,\n",
       "                1.4410e-02, 1.8150e-28, 2.2695e-31, 5.0349e-13, 1.5379e-02, 3.0528e-02,\n",
       "                1.1881e-02, 3.3711e-32, 2.2359e-31, 1.8240e-31, 3.1025e-03, 3.3752e-32,\n",
       "                1.8050e-31, 3.3665e-32, 2.5041e-31, 2.4609e-31, 3.3695e-32, 1.7498e-31,\n",
       "                1.5439e-31, 1.7720e-31, 1.4289e-31, 2.3557e-31, 2.4318e-31, 1.1172e-23,\n",
       "                1.4986e-31, 2.5635e-31, 2.3587e-31, 1.8751e-31, 2.5287e-31, 3.3731e-32,\n",
       "                5.6258e-04, 1.5076e-28, 2.7891e-02, 3.5023e-28, 2.2920e-31, 1.8145e-31,\n",
       "                2.6068e-02, 1.7773e-31, 3.1025e-02, 2.1039e-31, 1.7581e-31, 1.4955e-02,\n",
       "                2.3436e-31, 2.3562e-31, 2.5323e-31, 6.8635e-03, 2.4434e-31, 2.4094e-31,\n",
       "                1.3203e-23, 1.8598e-31, 3.0640e-31, 3.1017e-03, 2.5085e-31, 5.7093e-03,\n",
       "                1.8570e-23, 4.3644e-13, 1.7435e-31, 3.7919e-13, 1.5816e-31, 3.5610e-13,\n",
       "                2.4693e-31, 2.9330e-02, 4.3420e-13, 3.3379e-13, 2.3732e-31, 2.4169e-31,\n",
       "                2.1295e-02, 1.9321e-31, 3.1378e-02, 2.1814e-31, 3.1031e-03, 2.0138e-31,\n",
       "                3.3735e-32, 6.2717e-03, 3.0560e-18, 3.3691e-32, 3.9499e-31, 1.8303e-31,\n",
       "                3.3648e-32, 1.5413e-31, 1.8506e-28, 2.4397e-31, 6.2209e-31, 3.1379e-02,\n",
       "                2.5985e-31, 3.3682e-30, 3.3733e-32, 2.5893e-31, 6.2499e-31, 3.3619e-32,\n",
       "                2.5693e-31, 1.6769e-31, 1.8763e-31, 2.4998e-31, 1.6814e-31, 1.8603e-31,\n",
       "                5.5292e-04, 3.3647e-32, 2.4607e-31, 5.6856e-03, 1.7907e-31, 2.0911e-31,\n",
       "                2.4436e-31, 2.5907e-31, 2.4576e-31, 1.6789e-31, 2.5022e-31, 2.4933e-31,\n",
       "                2.2096e-23, 1.7959e-31, 1.0488e-31, 2.6748e-31, 1.8802e-31, 2.2156e-31,\n",
       "                1.7865e-31, 1.8952e-31, 3.3642e-32, 2.5729e-31, 1.9406e-31, 1.8619e-31,\n",
       "                1.4799e-31, 1.7721e-02, 2.2572e-31, 2.4406e-31, 2.4370e-31, 5.0572e-12,\n",
       "                3.3746e-32, 2.1937e-31, 1.0400e-31, 2.3004e-31, 1.5714e-31, 1.8418e-31,\n",
       "                1.1147e-23, 2.9536e-18, 2.1693e-31, 3.5551e-31, 3.3067e-13, 2.4597e-31,\n",
       "                8.3779e-03, 1.4328e-31, 2.1442e-31, 2.2120e-31, 1.3717e-02, 5.6345e-04,\n",
       "                3.8168e-13, 3.3738e-32])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0332]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.6682477881752034e-13, max_val=4.221776962280273)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Linear(\n",
       "    in_features=512, out_features=128, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.6433e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              1.6517e-04, 1.6506e-04, 1.6457e-04, 1.5547e-04, 6.1000e-05, 1.6428e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 1.6257e-04, 6.1000e-05, 1.6424e-04,\n",
       "              6.1000e-05, 6.1000e-05, 1.6413e-04, 6.1000e-05, 6.1000e-05, 1.6151e-04,\n",
       "              1.6467e-04, 6.1000e-05, 1.3620e-04, 1.4894e-04, 1.6403e-04, 1.6413e-04,\n",
       "              1.6256e-04, 1.6432e-04, 1.0666e-04, 6.1000e-05, 1.6516e-04, 6.1000e-05,\n",
       "              1.2825e-04, 6.1000e-05, 1.5251e-04, 6.1000e-05, 1.6511e-04, 1.4145e-04,\n",
       "              1.6413e-04, 6.1000e-05, 1.6444e-04, 1.6507e-04, 6.1000e-05, 6.1000e-05,\n",
       "              1.2589e-04, 6.1000e-05, 8.6527e-05, 6.1000e-05, 6.1000e-05, 1.0941e-04,\n",
       "              6.1000e-05, 1.6411e-04, 1.6414e-04, 1.6502e-04, 6.1000e-05, 1.6504e-04,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 8.5953e-05, 1.6522e-04,\n",
       "              6.1000e-05, 1.6438e-04, 6.1000e-05, 1.5277e-04, 1.2676e-04, 1.6505e-04,\n",
       "              6.1000e-05, 1.3349e-04, 1.6409e-04, 6.1000e-05, 6.1000e-05, 1.1947e-04,\n",
       "              6.1000e-05, 1.6400e-04, 6.1000e-05, 7.1629e-05, 1.6429e-04, 6.1000e-05,\n",
       "              1.6506e-04, 6.1000e-05, 1.6431e-04, 6.1000e-05, 6.1000e-05, 1.1606e-04,\n",
       "              1.2998e-04, 1.6312e-04, 6.1000e-05, 1.0394e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.6504e-04,\n",
       "              1.6434e-04, 1.2090e-04, 6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 1.6412e-04, 1.6517e-04, 6.1000e-05,\n",
       "              1.2974e-04, 6.1000e-05, 6.1000e-05, 1.6412e-04, 6.1000e-05, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05, 6.1000e-05, 6.1000e-05, 1.6512e-04, 6.1000e-05,\n",
       "              6.1000e-05, 6.1000e-05]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-2.8428e-12, -3.6520e-31, -3.7860e-31, -3.8799e-31, -3.8387e-31,\n",
       "                -3.8224e-31, -7.1483e-13, -3.0251e-12, -3.0925e-12, -1.1348e-12,\n",
       "                -3.7485e-31, -1.5013e-12, -3.8582e-31, -3.6394e-31, -3.9123e-31,\n",
       "                -2.7196e-12, -2.9185e-31, -1.2921e-12, -3.9097e-31, -3.8206e-31,\n",
       "                -3.1221e-12, -5.1487e-32, -3.7726e-31, -4.3836e-13, -1.7555e-12,\n",
       "                -3.4235e-31, -2.1141e-12, -2.7842e-12, -3.8864e-13, -2.8839e-12,\n",
       "                -1.2452e-12, -2.2733e-12, -2.0934e-12, -3.2790e-31, -3.1025e-12,\n",
       "                -2.6906e-31, -1.2741e-12, -3.3821e-31, -1.0217e-12, -3.4710e-31,\n",
       "                -3.1281e-12, -2.4849e-12, -3.0349e-12, -3.7858e-31, -4.7020e-13,\n",
       "                -3.1365e-12, -2.4589e-31, -3.8351e-31, -2.6045e-13, -8.0543e-32,\n",
       "                -6.3836e-13, -3.8685e-31, -3.8894e-31, -4.4974e-13, -3.8726e-31,\n",
       "                -3.0380e-12, -3.1220e-12, -1.3294e-12, -5.1489e-32, -2.6210e-12,\n",
       "                -3.1150e-31, -5.1135e-32, -2.7293e-31, -3.7349e-31, -3.5407e-13,\n",
       "                -3.1205e-12, -5.1474e-32, -3.8867e-13, -3.7576e-31, -1.8437e-12,\n",
       "                -3.0211e-13, -3.0680e-12, -3.8918e-31, -5.5797e-13, -9.5012e-13,\n",
       "                -3.8740e-31, -3.8481e-31, -2.8069e-13, -5.0935e-32, -3.7837e-13,\n",
       "                -3.7973e-31, -2.0485e-13, -3.0875e-12, -3.8320e-31, -3.0112e-12,\n",
       "                -3.8547e-31, -3.0627e-12, -5.1397e-32, -3.7908e-31, -4.7096e-13,\n",
       "                -1.2719e-12, -3.4459e-13, -3.9003e-31, -2.0781e-12, -3.8356e-31,\n",
       "                -3.8221e-31, -3.5135e-31, -3.8227e-31, -5.1492e-32, -5.1450e-32,\n",
       "                -3.8588e-31, -3.0516e-12, -2.2218e-12, -3.0023e-13, -1.6052e-31,\n",
       "                -3.7071e-31, -3.7363e-31, -3.3557e-31, -3.8627e-31, -3.6541e-31,\n",
       "                -3.7659e-31, -2.9591e-12, -3.1649e-12, -3.8343e-31, -5.3643e-13,\n",
       "                -3.4862e-31, -2.3961e-31, -2.1561e-12, -3.5621e-31, -7.6697e-32,\n",
       "                -5.0745e-32, -3.8936e-31, -3.8591e-31, -3.8529e-31, -3.1335e-12,\n",
       "                -3.8922e-31, -5.0890e-32, -3.8907e-31]), max_val=tensor([2.0870e-02, 3.4274e-31, 2.9467e-31, 3.4450e-31, 3.7015e-31, 2.8796e-31,\n",
       "                2.0976e-02, 2.0962e-02, 2.0900e-02, 1.9745e-02, 3.1489e-31, 2.0863e-02,\n",
       "                3.0112e-31, 2.7959e-31, 2.9577e-31, 2.0646e-02, 2.0959e-31, 2.0859e-02,\n",
       "                2.9998e-31, 2.9696e-31, 2.0844e-02, 5.1413e-32, 2.8538e-31, 2.0512e-02,\n",
       "                2.0913e-02, 2.7433e-31, 1.7297e-02, 1.8916e-02, 2.0832e-02, 2.0844e-02,\n",
       "                2.0645e-02, 2.0869e-02, 1.3545e-02, 3.6604e-31, 2.0975e-02, 3.6801e-31,\n",
       "                1.6288e-02, 2.7257e-31, 1.9368e-02, 2.4286e-31, 2.0969e-02, 1.7964e-02,\n",
       "                2.0845e-02, 3.3297e-31, 2.0884e-02, 2.0963e-02, 2.1855e-31, 3.2024e-31,\n",
       "                1.5988e-02, 6.8028e-32, 1.0989e-02, 2.7903e-31, 3.5662e-31, 1.3895e-02,\n",
       "                3.2485e-31, 2.0841e-02, 2.0846e-02, 2.0957e-02, 5.1218e-32, 2.0960e-02,\n",
       "                3.6608e-31, 5.1399e-32, 2.2325e-31, 2.8273e-31, 1.0916e-02, 2.0983e-02,\n",
       "                5.1370e-32, 2.0876e-02, 2.8241e-31, 1.9402e-02, 1.6098e-02, 2.0961e-02,\n",
       "                3.2823e-31, 1.6954e-02, 2.0840e-02, 2.9572e-31, 3.0240e-31, 1.5173e-02,\n",
       "                5.1457e-32, 2.0828e-02, 3.3466e-31, 9.0969e-03, 2.0865e-02, 2.8453e-31,\n",
       "                2.0963e-02, 2.9861e-31, 2.0867e-02, 5.1464e-32, 2.9079e-31, 1.4740e-02,\n",
       "                1.6508e-02, 2.0717e-02, 3.4474e-31, 1.3200e-02, 3.7476e-31, 3.4435e-31,\n",
       "                2.7172e-31, 3.1951e-31, 5.1040e-32, 5.1341e-32, 2.7984e-31, 2.0960e-02,\n",
       "                2.0872e-02, 1.5355e-02, 2.5562e-31, 2.7319e-31, 2.7205e-31, 3.5800e-31,\n",
       "                2.9415e-31, 2.7159e-31, 2.7143e-31, 2.0843e-02, 2.0976e-02, 3.4303e-31,\n",
       "                1.6477e-02, 2.4957e-31, 3.1244e-31, 2.0843e-02, 3.5355e-31, 7.7927e-32,\n",
       "                5.1413e-32, 2.8731e-31, 2.8962e-31, 3.5151e-31, 2.0970e-02, 2.8395e-31,\n",
       "                5.1305e-32, 3.5660e-31])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0197]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.918681846438617e-29, max_val=2.507524013519287)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Linear(\n",
       "    in_features=128, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-1.1501e-30]), max_val=tensor([0.1081]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1063]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=8.203132629394531, max_val=13.505492210388184)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62325\n",
      "Predict time: 10.143555164337158\n"
     ]
    }
   ],
   "source": [
    "model_qat.eval()\n",
    "y_pre = []\n",
    "T = []\n",
    "for _ in range(200):\n",
    "    times = 0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            stt= time.time()\n",
    "            outputs, _, _, _ = model_qat(inputs)\n",
    "            ent = time.time()\n",
    "            times+=ent-stt\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            y_pre.append(predicted)\n",
    "        T.append(times)\n",
    "T= np.array(T)\n",
    "accuracy0 = correct / total\n",
    "\n",
    "print(f'Accuracy: {accuracy0}')\n",
    "print(f'Predict time: {times}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save T:\n",
    "np.save('T_qat.npy', T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "T_qat = np.load('T_qat.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist([],bins=100,density =False)\n",
    "plt.hist(T_qat,bins=20,density = False, label = 'QAT')\n",
    "# plt.xlim(0,0.2)\n",
    "# plt.xscale('log')\n",
    "plt.title('Inference time')\n",
    "plt.legend()\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('time')\n",
    "plt.savefig('vt_qat.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lss = []\n",
    "for i in range(len(losses)):\n",
    "    lss.append(losses[i])\n",
    "plt.plot(lss)\n",
    "plt.title('BCE Loss')\n",
    "plt.savefig('Loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelQ(\n",
       "  (quant): Quantize(scale=tensor([0.2662]), zero_point=tensor([30]), dtype=torch.quint8)\n",
       "  (layer0): QuantizedLinear(in_features=140, out_features=1024, scale=0.04846855625510216, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (layer1): QuantizedLinear(in_features=1024, out_features=512, scale=0.03324233740568161, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (layer2): QuantizedLinear(in_features=512, out_features=128, scale=0.019744284451007843, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (layer3): QuantizedLinear(in_features=128, out_features=1, scale=0.10634245723485947, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_qat.eval()\n",
    "model_qat.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "model_qat_prepared = torch.quantization.prepare(model_qat, inplace=False)\n",
    "quantized_model = torch.quantization.convert(model_qat_prepared, inplace=False)\n",
    "quantized_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.62325\n",
      "Predict time: 0.0633857250213623\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(200):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    Qtimes = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            stt = time.time()\n",
    "            outputs, _,  _ , _ = quantized_model(inputs)\n",
    "            ent = time.time()\n",
    "            Qtimes+=ent-stt\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    QT.append(Qtimes)\n",
    "QT=np.array(QT)\n",
    "accuracy2 = correct / total\n",
    "print(f'Accuracy = {accuracy2}')\n",
    "print(f'Predict time: {Qtimes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save T:\n",
    "np.save('T_qq.npy', QT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "T_qq = np.load('T_qq.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T_qq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist([],bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,density \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mhist(\u001b[43mT_qq\u001b[49m,bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,density \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQAT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plt.xlim(0,0.2)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# plt.xscale('log')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInference time\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T_qq' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([],bins=100,density =False)\n",
    "plt.hist(T_qq,bins=20,density = False, label = 'QAT')\n",
    "# plt.xlim(0,0.2)\n",
    "# plt.xscale('log')\n",
    "plt.title('Inference time')\n",
    "plt.legend()\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('time')\n",
    "plt.savefig('vt_qat.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_qat.eval()\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model_qat,  \n",
    "    {nn.Linear},  \n",
    "    dtype=torch.qint8\n",
    ")\n",
    "quantized_model.eval()\n",
    "QT = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FusedMovingAvgObsFakeQuantize' object has no attribute 'activation_post_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m      7\u001b[0m     stt \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m     outputs, _,  _ , _ \u001b[38;5;241m=\u001b[39m \u001b[43mquantized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     ent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m     Qtimes\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39ment\u001b[38;5;241m-\u001b[39mstt\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mBinaryClassificationModelQ.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m timings[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquant_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     19\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x))\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/nn/qat/modules/linear.py:41\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_fake_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/quantization/fake_quantize.py:346\u001b[0m, in \u001b[0;36mFusedMovingAvgObsFakeQuantize.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfused_moving_avg_obs_fake_quant(\n\u001b[1;32m    343\u001b[0m         X,\n\u001b[1;32m    344\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobserver_enabled,\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfake_quant_enabled,\n\u001b[0;32m--> 346\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_post_process\u001b[49m\u001b[38;5;241m.\u001b[39mmin_val,\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process\u001b[38;5;241m.\u001b[39mmax_val,\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale,\n\u001b[1;32m    349\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point,\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process\u001b[38;5;241m.\u001b[39maveraging_constant,\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process\u001b[38;5;241m.\u001b[39mquant_min,\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_post_process\u001b[38;5;241m.\u001b[39mquant_max,\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch_axis,\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_per_channel,\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_symmetric_quant,\n\u001b[1;32m    356\u001b[0m     )\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FusedMovingAvgObsFakeQuantize' object has no attribute 'activation_post_process'"
     ]
    }
   ],
   "source": [
    "\n",
    "for _ in range(200):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    Qtimes = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            stt = time.time()\n",
    "            outputs, _,  _ , _ = quantized_model(inputs)\n",
    "            ent = time.time()\n",
    "            Qtimes+=ent-stt\n",
    "            predicted = (outputs.squeeze() > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    QT.append(Qtimes)\n",
    "QT=np.array(QT)\n",
    "accuracy2 = correct / total\n",
    "print(f'Accuracy = {accuracy2}')\n",
    "print(f'Predict time: {Qtimes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 1024\n",
    "\n",
    "df = pd.read_parquet('df.parquet')\n",
    "X = pd.read_parquet('x.parquet').values  \n",
    "df['proc'] = df['proc'].apply(lambda x: 1 if x != 0 else 0)\n",
    "y = df['proc'].values  \n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "layers = [1024, 512, 128]\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer0): Linear(\n",
       "    in_features=140, out_features=1024, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Linear(\n",
       "    in_features=1024, out_features=512, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Linear(\n",
       "    in_features=512, out_features=128, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Linear(\n",
       "    in_features=128, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryClassificationModelQ(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassificationModelQ, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.layer0 = nn.Linear(X.shape[1], layers[0])\n",
    "        self.layer1 = nn.Linear(layers[0], layers[1])\n",
    "        self.layer2 = nn.Linear(layers[1], layers[2])\n",
    "        self.layer3 = nn.Linear(layers[2], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        timings = {}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        x = self.quant(x)\n",
    "        timings['quant_time'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = torch.relu(self.layer0(x))\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        timings['fc_time'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        x = self.dequant(x)\n",
    "        timings['dequant_time'] = time.time() - start_time\n",
    "\n",
    "        return x, timings['quant_time'], timings['fc_time'], timings['dequant_time']\n",
    "\n",
    "modelQ = BinaryClassificationModelQ()\n",
    "modelQ.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(modelQ, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 1/20 [00:34<10:47, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7136371630303403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 2/20 [01:11<10:46, 35.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.5702622748435813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 3/20 [01:45<09:54, 34.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2.0709333698800267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 4/20 [02:19<09:16, 34.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.0712458706916648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 5/20 [02:55<08:46, 35.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2.07147615513903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 6/20 [03:28<08:04, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 2.0717558531050986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 7/20 [04:03<07:31, 34.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 2.07114729221831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 8/20 [04:37<06:53, 34.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 2.071393979356644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 9/20 [05:13<06:23, 34.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 2.0714104277022343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 10/20 [05:48<05:48, 34.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.071772364859885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  55%|█████▌    | 11/20 [06:22<05:11, 34.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 2.0715255737304688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 12/20 [06:57<04:38, 34.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 2.0709334916256843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 13/20 [07:33<04:04, 34.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 2.0717229335866074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  65%|██████▌   | 13/20 [07:58<04:17, 36.79s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m losses, timing, model_qat \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[60], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, lr, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "losses, timing, model_qat = train_model(modelQ, train_loader, lr=0.001, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    start_time = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs, _, _, _ = model(inputs)\n",
    "            predicted = torch.round(torch.sigmoid(outputs))\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "\n",
    "    end_time = time.time()  # End time\n",
    "    evaluation_time = end_time - start_time\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Time: {evaluation_time}s')\n",
    "    \n",
    "    return accuracy, evaluation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89975\n",
      "Time: 4.110225439071655s\n"
     ]
    }
   ],
   "source": [
    "accQ, tQ = eval(model_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelQ(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer0): Linear(\n",
       "    in_features=140, out_features=32, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer1): Linear(\n",
       "    in_features=32, out_features=16, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Linear(\n",
       "    in_features=16, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryClassificationModelQ(nn.Module):\n",
    "    def __init__(self, num_features, mask_size=None, mask=None):\n",
    "        super(BinaryClassificationModelQ, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.layer0 = nn.Linear(num_features, layers[0])\n",
    "        self.layer1 = nn.Linear(layers[0], layers[1])\n",
    "        self.layer2 = nn.Linear(layers[1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        timings = {}\n",
    "        \n",
    "        start_time = time.time()\n",
    "        x = self.quant(x)\n",
    "        timings['quant_time'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = torch.relu(self.layer0(x))\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        timings['fc_time'] = time.time() - start_time\n",
    "\n",
    "        start_time = time.time()\n",
    "        x = self.dequant(x)\n",
    "        timings['dequant_time'] = time.time() - start_time\n",
    "\n",
    "        return x, timings['quant_time'], timings['fc_time'], timings['dequant_time']\n",
    "\n",
    "modelQ = BinaryClassificationModelQ(X.shape[1])\n",
    "modelQ.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(modelQ, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, lr=0.001, num_epochs=50):\n",
    "    model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Combines Sigmoid + BCELoss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    timing = {'quant_time': 0, 'fc_time': 0, 'dequant_time': 0}\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        epoch_loss = 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs, quant_time, fc_time,dequant_time = model(inputs)\n",
    "            \n",
    "            # Accumulate the timing information\n",
    "            timing['quant_time'] += quant_time\n",
    "            timing['fc_time'] += fc_time\n",
    "            timing['dequant_time'] += dequant_time\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        # print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "    \n",
    "    return losses, timing, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 50/50 [07:41<00:00,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing: {'quant_time': 47.3816077709198, 'fc_time': 167.51681351661682, 'dequant_time': 0.04328298568725586}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "losses, timing, model_qat = train_model(modelQ, train_loader, lr=0.001, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f226616e210>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quant_time': 47.3816077709198,\n",
       " 'fc_time': 167.51681351661682,\n",
       " 'dequant_time': 0.04328298568725586}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 12/12 [00:02<00:00,  5.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_qat.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        outputs, _, _, _ =  model_qat(inputs)\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.squeeze() == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.8446474577685765\n"
     ]
    }
   ],
   "source": [
    "y_pre = []\n",
    "y_labe = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = modelQ(inputs)\n",
    "        y_pre=np.append(y_pre,outputs.numpy())\n",
    "        y_labe=np.append(y_labe,labels.numpy())\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_labe, y_pre)\n",
    "print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: fp32 \t Size(KB): 40.564\n",
      "model: int8 \t Size(KB): 40.564\n",
      "1.00 times smaller\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def model_size(model, label=' '):\n",
    "    torch.save(model_qat.state_dict(),\"/vols/cms/yhe4823/Acc/mymodel.p\")\n",
    "    size = os.path.getsize(\"/vols/cms/yhe4823/Acc/mymodel.p\")\n",
    "    print(\"model:\",label,'\\t','Size(KB):',size/1e3)\n",
    "    os.remove(\"/vols/cms/yhe4823/Acc/mymodel.p\")\n",
    "    return size\n",
    "# compare the size\n",
    "f = model_size(model, \"fp32\")\n",
    "q = model_size(model_qat, \"int8\")\n",
    "print(\"{0:.2f} times smaller\".format(f/q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = BinaryClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "losses = []\n",
    "stt = time.time()\n",
    "with tqdm(range(num_epochs)) as t:\n",
    "    for epoch in t:\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        losses.append(loss)\n",
    "        t.set_postfix(train_loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitwise Encoding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitMask(nn.Module):\n",
    "    def __init__(self, size, mask=None):\n",
    "        super(BitMask, self).__init__()\n",
    "        if mask is None:\n",
    "            self.mask = torch.ones(size, dtype=torch.float32)\n",
    "        else:\n",
    "            assert mask.size() == size\n",
    "            self.mask = mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassificationModelB(nn.Module):\n",
    "    def __init__(self, num_features, mask_size=None, mask=None):\n",
    "        super(BinaryClassificationModelB, self).__init__()\n",
    "        self.layer0 = nn.Linear(num_features, layers[0])\n",
    "        self.layer1 = nn.Linear(layers[0], layers[1])\n",
    "        self.bitmask = BitMask(layers[1], mask)\n",
    "        self.layer2 = nn.Linear(layers[1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer0(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.bitmask(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:14<00:00,  6.37s/it, train_loss=0.164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.91775\n"
     ]
    }
   ],
   "source": [
    "model_B = BinaryClassificationModelB(X.shape[1])\n",
    "loss_B, model_b = Quantrain(model_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.966748375951078\n"
     ]
    }
   ],
   "source": [
    "y_pre = []\n",
    "y_labe = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_b(inputs)\n",
    "        y_pre=np.append(y_pre,outputs.numpy())\n",
    "        y_labe=np.append(y_labe,labels.numpy())\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_labe, y_pre)\n",
    "print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelB(\n",
       "  (layer0): Linear(in_features=140, out_features=32, bias=True)\n",
       "  (layer1): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (bitmask): BitMask()\n",
       "  (layer2): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning \n",
    "Add pruning to the qunatised model: used to reduce the size of a neural network by removing weights that have little to no effect on the output, potentially improving inference times and reducing the model's memory footprint without significantly impacting its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pruning(model):\n",
    "    # Specify the pruning amount as a fraction of the existing connections\n",
    "    pruning_amount = 0.1\n",
    "    # Apply pruning to layer0 and layer1\n",
    "    prune.l1_unstructured(model.layer0, name='weight', amount=pruning_amount)\n",
    "    prune.l1_unstructured(model.layer1, name='weight', amount=pruning_amount)\n",
    "    print(model.layer0.weight) \n",
    "    \n",
    "    # Make pruning permanent\n",
    "    prune.remove(model.layer0, 'weight')\n",
    "    prune.remove(model.layer1, 'weight')\n",
    "\n",
    "model_P = BinaryClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [01:03<03:40,  7.11s/it, train_loss=0.333]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ BinaryClassificationModel(\n",
      "  (layer0): Linear(in_features=140, out_features=32, bias=True)\n",
      "  (layer1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [02:14<02:26,  6.98s/it, train_loss=0.241]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ BinaryClassificationModel(\n",
      "  (layer0): Linear(in_features=140, out_features=32, bias=True)\n",
      "  (layer1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [03:21<01:11,  6.48s/it, train_loss=0.183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ BinaryClassificationModel(\n",
      "  (layer0): Linear(in_features=140, out_features=32, bias=True)\n",
      "  (layer1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [04:28<00:06,  6.57s/it, train_loss=0.21] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++ BinaryClassificationModel(\n",
      "  (layer0): Linear(in_features=140, out_features=32, bias=True)\n",
      "  (layer1): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (layer2): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:34<00:00,  6.87s/it, train_loss=0.235]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.907\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_P.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "losses = []\n",
    "stt = time.time()\n",
    "with tqdm(range(num_epochs)) as t:\n",
    "    for epoch in t:\n",
    "        if epoch % 10 == 9:  # Apply pruning at every 10th epoch (epoch number starts from 0)\n",
    "            Pruning(model_P)\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            outputs = model_P(inputs)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        losses.append(loss)\n",
    "        # print(f'Epoch {epoch+1}, Estimate:{dtt}, Loss: {loss.item()}')\n",
    "        t.set_postfix(train_loss=loss.item())\n",
    "\n",
    "model_P.eval()\n",
    "y_pre = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_P(inputs)\n",
    "        predicted = (outputs.squeeze() > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        y_pre.append(predicted)\n",
    "        \n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 0.9672297799169829\n"
     ]
    }
   ],
   "source": [
    "AUC_P = AUC(model_P,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning + QAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qat frist and then prune again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def apply_pruning(model):\n",
    "    # Apply pruning to layer0 and layer1\n",
    "    # Removing 30% of the connections based on the absolute value of the weights\n",
    "    prune.l1_unstructured(model.layer0, name=\"weight\", amount=0.3)\n",
    "    prune.l1_unstructured(model.layer1, name=\"weight\", amount=0.3)\n",
    "\n",
    "    # Make the pruning permanent\n",
    "    prune.remove(model.layer0, 'weight')\n",
    "    prune.remove(model.layer1, 'weight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryClassificationModelQAT(\n",
       "  (layer0): LinearReLU(\n",
       "    in_features=140, out_features=32, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu0): Identity()\n",
       "  (layer1): LinearReLU(\n",
       "    in_features=32, out_features=16, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (relu1): Identity()\n",
       "  (layer2): Linear(\n",
       "    in_features=16, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (sigmoid): Sigmoid(\n",
       "    (activation_post_process): FixedQParamsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "      (activation_post_process): FixedQParamsObserver()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "\n",
    "class BinaryClassificationModelQAT(nn.Module):\n",
    "    def __init__(self, input_features, hidden_layers):\n",
    "        super(BinaryClassificationModelQAT, self).__init__()\n",
    "        self.layer0 = nn.Linear(input_features, hidden_layers[0])\n",
    "        self.relu0 = nn.ReLU()  # Explicitly define ReLU as relu0\n",
    "        self.layer1 = nn.Linear(hidden_layers[0], hidden_layers[1])\n",
    "        self.relu1 = nn.ReLU()  # Explicitly define ReLU as relu1\n",
    "        self.layer2 = nn.Linear(hidden_layers[1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu0(self.layer0(x))\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.sigmoid(self.layer2(x))\n",
    "        return x\n",
    "\n",
    "    def fuse_model(self):\n",
    "        torch.quantization.fuse_modules(self, [['layer0', 'relu0'], ['layer1', 'relu1']], inplace=True)\n",
    "\n",
    "# Assuming the model instance\n",
    "model_qat = BinaryClassificationModelQAT(input_features=X.shape[1], hidden_layers=[32,16])\n",
    "\n",
    "# Fuse the model to combine convolution and batch normalization layers where applicable\n",
    "model_qat.fuse_model()\n",
    "\n",
    "# Switch to QAT mode\n",
    "model_qat.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# Prepare the model for QAT\n",
    "torch.quantization.prepare_qat(model_qat, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Estimate:602.6464347839355, Loss: 0.6445045471191406\n",
      "Epoch 2, Estimate:495.1404836177826, Loss: 0.5911950469017029\n",
      "Epoch 3, Estimate:448.2152536710104, Loss: 0.5419753789901733\n",
      "Epoch 4, Estimate:420.34929299354553, Loss: 0.5422425270080566\n",
      "Epoch 5, Estimate:401.56582474708557, Loss: 0.5179789662361145\n",
      "Epoch 6, Estimate:388.44745572408044, Loss: 0.5270822644233704\n",
      "Epoch 7, Estimate:375.97768797193254, Loss: 0.511357843875885\n",
      "Epoch 8, Estimate:360.4029293060303, Loss: 0.5035499334335327\n",
      "Epoch 9, Estimate:346.72257810168793, Loss: 0.500896155834198\n",
      "Epoch 10, Estimate:335.5594825744629, Loss: 0.517208993434906\n",
      "Epoch 11, Estimate:322.91367723725057, Loss: 0.5052278637886047\n",
      "Epoch 12, Estimate:310.0045808156332, Loss: 0.5045841932296753\n",
      "Epoch 13, Estimate:298.55828899603625, Loss: 0.4895503520965576\n",
      "Epoch 14, Estimate:287.1485148838588, Loss: 0.5005485415458679\n",
      "Epoch 15, Estimate:275.2872506777445, Loss: 0.4787898659706116\n",
      "Epoch 16, Estimate:263.30379831790924, Loss: 0.48470738530158997\n",
      "Epoch 17, Estimate:252.2055320739746, Loss: 0.4704304337501526\n",
      "Epoch 18, Estimate:243.0688228872087, Loss: 0.46758154034614563\n",
      "Epoch 19, Estimate:232.4604617169029, Loss: 0.4521673619747162\n",
      "Epoch 20, Estimate:220.82584357261658, Loss: 0.47294020652770996\n",
      "Epoch 21, Estimate:209.16066082318625, Loss: 0.4835366904735565\n",
      "Epoch 22, Estimate:198.1609426845204, Loss: 0.46378859877586365\n",
      "Epoch 23, Estimate:187.52177822071573, Loss: 0.4456418454647064\n",
      "Epoch 24, Estimate:176.36074574788412, Loss: 0.4327963888645172\n",
      "Epoch 25, Estimate:165.41181135177612, Loss: 0.4488803446292877\n",
      "Epoch 26, Estimate:154.48992841060345, Loss: 0.4022562801837921\n",
      "Epoch 27, Estimate:143.4308492342631, Loss: 0.41650786995887756\n",
      "Epoch 28, Estimate:132.5858063016619, Loss: 0.4213634431362152\n",
      "Epoch 29, Estimate:121.39330980695527, Loss: 0.4384552538394928\n",
      "Epoch 30, Estimate:110.18788488705952, Loss: 0.4086053967475891\n",
      "Epoch 31, Estimate:99.22925986013104, Loss: 0.41303014755249023\n",
      "Epoch 32, Estimate:88.18715995550156, Loss: 0.40944215655326843\n",
      "Epoch 33, Estimate:77.05938280712475, Loss: 0.390974223613739\n",
      "Epoch 34, Estimate:66.26247937539044, Loss: 0.36044177412986755\n",
      "Epoch 35, Estimate:55.55175641604833, Loss: 0.36361899971961975\n",
      "Epoch 36, Estimate:44.40495464536879, Loss: 0.3552436828613281\n",
      "Epoch 37, Estimate:33.254178498242354, Loss: 0.36344578862190247\n",
      "Epoch 38, Estimate:22.16029301442598, Loss: 0.3398391306400299\n",
      "Epoch 39, Estimate:11.062441990925716, Loss: 0.3343983590602875\n",
      "Epoch 40, Estimate:0.0, Loss: 0.3340490460395813\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear_relu' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_relu' is only available for these backends: [QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nQuantizedCPU: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/native/quantized/cpu/qlinear.cpp:990 [kernel]\nBackendSelect: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:30 [backend fallback]\nAutogradCPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:34 [backend fallback]\nAutogradCUDA: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:42 [backend fallback]\nAutogradXLA: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:46 [backend fallback]\nAutogradMPS: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:54 [backend fallback]\nAutogradXPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:38 [backend fallback]\nAutogradHPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradLazy: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:50 [backend fallback]\nAutogradMeta: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:58 [backend fallback]\nTracer: registered at /croot/pytorch_1686931851744/work/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses, modelQ \u001b[38;5;241m=\u001b[39m \u001b[43mQuantrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_qat\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 85\u001b[0m, in \u001b[0;36mQuantrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     83\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m---> 85\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39msqueeze() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     87\u001b[0m     total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mBinaryClassificationModelQAT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu0(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x))\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/ao/nn/intrinsic/quantized/modules/linear_relu.py:36\u001b[0m, in \u001b[0;36mLinearReLU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vols/cms/yhe4823/Acc/env/envs/env/lib/python3.11/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear_relu' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear_relu' is only available for these backends: [QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nQuantizedCPU: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/native/quantized/cpu/qlinear.cpp:990 [kernel]\nBackendSelect: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:30 [backend fallback]\nAutogradCPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:34 [backend fallback]\nAutogradCUDA: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:42 [backend fallback]\nAutogradXLA: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:46 [backend fallback]\nAutogradMPS: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:54 [backend fallback]\nAutogradXPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:38 [backend fallback]\nAutogradHPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradLazy: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:50 [backend fallback]\nAutogradMeta: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/VariableFallbackKernel.cpp:58 [backend fallback]\nTracer: registered at /croot/pytorch_1686931851744/work/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/autocast_mode.cpp:487 [backend fallback]\nAutocastCUDA: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at /croot/pytorch_1686931851744/work/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at /croot/pytorch_1686931851744/work/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "losses, modelQ = Quantrain(model_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = []\n",
    "y_labe = []\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model_qat(inputs)\n",
    "        y_pre=np.append(y_pre,outputs.numpy())\n",
    "        y_labe=np.append(y_labe,labels.numpy())\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc = roc_auc_score(y_labe, y_pre)\n",
    "print(f'AUC = {auc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary nets\n",
    "- Binary Connect: \n",
    "- BNN\n",
    "- XNOR_Net\n",
    "\n",
    "Zeros: \n",
    "- Relu maps the neg values to zeros, then may lead to the some losses\n",
    "\n",
    "TO do:\n",
    "- Enconde bitmask\n",
    "- Run-length encoding\n",
    "- CSC encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the time sepeartely, inversion time and computational time \n",
    "plot of number of the parameters and the size / time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leadPhotonEn</th>\n",
       "      <th>leadPhotonMass</th>\n",
       "      <th>leadPhotonPt</th>\n",
       "      <th>leadPhotonEta</th>\n",
       "      <th>leadPhotonPhi</th>\n",
       "      <th>leadPhotonIDMVA</th>\n",
       "      <th>leadPhotonSigmaE</th>\n",
       "      <th>leadPhotonHoE</th>\n",
       "      <th>leadPhotonPfRelIsoAll</th>\n",
       "      <th>leadPhotonPfRelIsoChg</th>\n",
       "      <th>...</th>\n",
       "      <th>leadJetDiphoDEta</th>\n",
       "      <th>subleadJetDiphoDPhi</th>\n",
       "      <th>subleadJetDiphoDEta</th>\n",
       "      <th>nSoftJets</th>\n",
       "      <th>metPt</th>\n",
       "      <th>metPhi</th>\n",
       "      <th>metSumET</th>\n",
       "      <th>metSignificance</th>\n",
       "      <th>weight</th>\n",
       "      <th>proc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111356</th>\n",
       "      <td>77.388908</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>57.133667</td>\n",
       "      <td>0.818970</td>\n",
       "      <td>-1.590332</td>\n",
       "      <td>0.914551</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027066</td>\n",
       "      <td>0.027066</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.645295</td>\n",
       "      <td>0.412073</td>\n",
       "      <td>-0.237702</td>\n",
       "      <td>6.0</td>\n",
       "      <td>22.016317</td>\n",
       "      <td>-0.238159</td>\n",
       "      <td>1211.0</td>\n",
       "      <td>0.869141</td>\n",
       "      <td>1.312993e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708637</th>\n",
       "      <td>129.196991</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>129.048645</td>\n",
       "      <td>0.047943</td>\n",
       "      <td>1.322998</td>\n",
       "      <td>0.972168</td>\n",
       "      <td>1.296875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605590</td>\n",
       "      <td>2.341702</td>\n",
       "      <td>0.726195</td>\n",
       "      <td>6.0</td>\n",
       "      <td>77.595505</td>\n",
       "      <td>0.022762</td>\n",
       "      <td>775.5</td>\n",
       "      <td>21.265625</td>\n",
       "      <td>9.591657e-11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846317</th>\n",
       "      <td>129.661087</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>124.301880</td>\n",
       "      <td>-0.292603</td>\n",
       "      <td>-2.690918</td>\n",
       "      <td>0.744141</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>0.015930</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.327674</td>\n",
       "      <td>0.620501</td>\n",
       "      <td>0.540246</td>\n",
       "      <td>6.0</td>\n",
       "      <td>17.334482</td>\n",
       "      <td>1.477539</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>0.518555</td>\n",
       "      <td>1.260631e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577373</th>\n",
       "      <td>89.011086</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>89.006615</td>\n",
       "      <td>-0.010021</td>\n",
       "      <td>-2.511719</td>\n",
       "      <td>0.708984</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.119852</td>\n",
       "      <td>0.106323</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>0.977051</td>\n",
       "      <td>7.116355e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213346</th>\n",
       "      <td>155.278717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.924973</td>\n",
       "      <td>-0.314026</td>\n",
       "      <td>2.483398</td>\n",
       "      <td>0.963379</td>\n",
       "      <td>1.531250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.512229</td>\n",
       "      <td>-1.419967</td>\n",
       "      <td>3.073708</td>\n",
       "      <td>6.0</td>\n",
       "      <td>108.228378</td>\n",
       "      <td>-1.519043</td>\n",
       "      <td>2620.0</td>\n",
       "      <td>8.085938</td>\n",
       "      <td>6.632386e-11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667971</th>\n",
       "      <td>73.866203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.475899</td>\n",
       "      <td>-0.103027</td>\n",
       "      <td>-1.945801</td>\n",
       "      <td>0.977539</td>\n",
       "      <td>0.757812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.846709</td>\n",
       "      <td>-2.697640</td>\n",
       "      <td>2.885161</td>\n",
       "      <td>6.0</td>\n",
       "      <td>33.445145</td>\n",
       "      <td>-2.115723</td>\n",
       "      <td>2037.0</td>\n",
       "      <td>1.334961</td>\n",
       "      <td>9.410649e-11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316710</th>\n",
       "      <td>107.925034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>72.095215</td>\n",
       "      <td>-0.959717</td>\n",
       "      <td>-2.927734</td>\n",
       "      <td>0.838379</td>\n",
       "      <td>1.453125</td>\n",
       "      <td>0.011169</td>\n",
       "      <td>0.017436</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.165186</td>\n",
       "      <td>-2.899414</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>1.063477</td>\n",
       "      <td>9.897219e-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298067</th>\n",
       "      <td>149.264282</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>89.225029</td>\n",
       "      <td>-1.103271</td>\n",
       "      <td>1.434570</td>\n",
       "      <td>0.954590</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>39.433796</td>\n",
       "      <td>-2.859375</td>\n",
       "      <td>753.5</td>\n",
       "      <td>4.476562</td>\n",
       "      <td>1.483928e-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053454</th>\n",
       "      <td>112.139229</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>111.939842</td>\n",
       "      <td>0.059677</td>\n",
       "      <td>-3.135254</td>\n",
       "      <td>0.981445</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>0.009338</td>\n",
       "      <td>0.007979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912704</td>\n",
       "      <td>-0.989709</td>\n",
       "      <td>-2.197648</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50.148216</td>\n",
       "      <td>2.472168</td>\n",
       "      <td>2754.0</td>\n",
       "      <td>2.589844</td>\n",
       "      <td>1.436982e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646554</th>\n",
       "      <td>354.755737</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>102.843002</td>\n",
       "      <td>1.909668</td>\n",
       "      <td>2.664062</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>0.024516</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.302818</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>32.015461</td>\n",
       "      <td>-1.243408</td>\n",
       "      <td>1444.0</td>\n",
       "      <td>1.923828</td>\n",
       "      <td>2.535550e-06</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 143 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         leadPhotonEn  leadPhotonMass  leadPhotonPt  leadPhotonEta  \\\n",
       "1111356     77.388908        0.000001     57.133667       0.818970   \n",
       "708637     129.196991       -0.000003    129.048645       0.047943   \n",
       "846317     129.661087        0.000002    124.301880      -0.292603   \n",
       "2577373     89.011086        0.000001     89.006615      -0.010021   \n",
       "213346     155.278717        0.000000    147.924973      -0.314026   \n",
       "...               ...             ...           ...            ...   \n",
       "667971      73.866203        0.000000     73.475899      -0.103027   \n",
       "316710     107.925034        0.000000     72.095215      -0.959717   \n",
       "298067     149.264282        0.000002     89.225029      -1.103271   \n",
       "2053454    112.139229       -0.000002    111.939842       0.059677   \n",
       "646554     354.755737        0.000008    102.843002       1.909668   \n",
       "\n",
       "         leadPhotonPhi  leadPhotonIDMVA  leadPhotonSigmaE  leadPhotonHoE  \\\n",
       "1111356      -1.590332         0.914551          0.937500       0.000000   \n",
       "708637        1.322998         0.972168          1.296875       0.000000   \n",
       "846317       -2.690918         0.744141          1.562500       0.015930   \n",
       "2577373      -2.511719         0.708984          0.890625       0.000000   \n",
       "213346        2.483398         0.963379          1.531250       0.000000   \n",
       "...                ...              ...               ...            ...   \n",
       "667971       -1.945801         0.977539          0.757812       0.000000   \n",
       "316710       -2.927734         0.838379          1.453125       0.011169   \n",
       "298067        1.434570         0.954590          3.406250       0.000000   \n",
       "2053454      -3.135254         0.981445          1.375000       0.009338   \n",
       "646554        2.664062         0.968750          8.500000       0.029602   \n",
       "\n",
       "         leadPhotonPfRelIsoAll  leadPhotonPfRelIsoChg  ...  leadJetDiphoDEta  \\\n",
       "1111356               0.027066               0.027066  ...         -1.645295   \n",
       "708637                0.000000               0.000000  ...          0.605590   \n",
       "846317                0.021015               0.021015  ...         -1.327674   \n",
       "2577373               0.000000               0.000000  ...       -999.000000   \n",
       "213346                0.008058               0.008058  ...         -2.512229   \n",
       "...                        ...                    ...  ...               ...   \n",
       "667971                0.000000               0.000000  ...          1.846709   \n",
       "316710                0.017436               0.001089  ...       -999.000000   \n",
       "298067                0.000000               0.000000  ...       -999.000000   \n",
       "2053454               0.007979               0.000000  ...          0.912704   \n",
       "646554                0.024516               0.000000  ...         -4.302818   \n",
       "\n",
       "         subleadJetDiphoDPhi  subleadJetDiphoDEta  nSoftJets       metPt  \\\n",
       "1111356             0.412073            -0.237702        6.0   22.016317   \n",
       "708637              2.341702             0.726195        6.0   77.595505   \n",
       "846317              0.620501             0.540246        6.0   17.334482   \n",
       "2577373          -999.000000          -999.000000        6.0   20.119852   \n",
       "213346             -1.419967             3.073708        6.0  108.228378   \n",
       "...                      ...                  ...        ...         ...   \n",
       "667971             -2.697640             2.885161        6.0   33.445145   \n",
       "316710           -999.000000          -999.000000        6.0   19.165186   \n",
       "298067           -999.000000          -999.000000        6.0   39.433796   \n",
       "2053454            -0.989709            -2.197648        6.0   50.148216   \n",
       "646554           -999.000000          -999.000000        6.0   32.015461   \n",
       "\n",
       "           metPhi  metSumET  metSignificance        weight  proc  \n",
       "1111356 -0.238159    1211.0         0.869141  1.312993e-06     0  \n",
       "708637   0.022762     775.5        21.265625  9.591657e-11     1  \n",
       "846317   1.477539    1763.0         0.518555  1.260631e-06     0  \n",
       "2577373  0.106323    1245.0         0.977051  7.116355e-06     0  \n",
       "213346  -1.519043    2620.0         8.085938  6.632386e-11     1  \n",
       "...           ...       ...              ...           ...   ...  \n",
       "667971  -2.115723    2037.0         1.334961  9.410649e-11     1  \n",
       "316710  -2.899414    1018.0         1.063477  9.897219e-08     1  \n",
       "298067  -2.859375     753.5         4.476562  1.483928e-08     1  \n",
       "2053454  2.472168    2754.0         2.589844  1.436982e-06     0  \n",
       "646554  -1.243408    1444.0         1.923828  2.535550e-06     0  \n",
       "\n",
       "[60000 rows x 143 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
